
@inproceedings{wobbrock_gestures_2007,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '07},
	title = {Gestures Without Libraries, Toolkits or Training: A \$1 Recognizer for User Interface Prototypes},
	isbn = {978-1-59593-679-0},
	shorttitle = {Gestures Without Libraries, Toolkits or Training},
	url = {http://doi.acm.org/10.1145/1294211.1294238},
	doi = {10.1145/1294211.1294238},
	abstract = {Although mobile, tablet, large display, and tabletop computers increasingly present opportunities for using pen, finger, and wand gestures in user interfaces, implementing gesture recognition largely has been the privilege of pattern matching experts, not user interface prototypers. Although some user interface libraries and toolkits offer gesture recognizers, such infrastructure is often unavailable in design-oriented environments like Flash, scripting environments like {JavaScript}, or brand new off-desktop prototyping environments. To enable novice programmers to incorporate gestures into their {UI} prototypes, we present a "\$1 recognizer" that is easy, cheap, and usable almost anywhere in about 100 lines of code. In a study comparing our \$1 recognizer, Dynamic Time Warping, and the Rubine classifier on user-supplied gestures, we found that \$1 obtains over 97\% accuracy with only 1 loaded template and 99\% accuracy with 3+ loaded templates. These results were nearly identical to {DTW} and superior to Rubine. In addition, we found that medium-speed gestures, in which users balanced speed and accuracy, were recognized better than slow or fast gestures for all three recognizers. We also discuss the effect that the number of templates or training examples has on recognition, the score falloff along recognizers' N-best lists, and results for individual gestures. We include detailed pseudocode of the \$1 recognizer to aid development, inspection, extension, and testing.},
	urldate = {2014-04-11},
	booktitle = {Proceedings of the 20th Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Wobbrock, Jacob O. and Wilson, Andrew D. and Li, Yang},
	year = {2007},
	keywords = {dynamic time warping, gesture recognition, marks, rapid prototyping, recognition rates, rubine, statistical classifiers, strokes, symbols, unistrokes, user interfaces},
	pages = {159--168},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\5ZX89Q53\\Wobbrock et al. - 2007 - Gestures Without Libraries, Toolkits or Training .pdf:application/pdf}
}

@article{khoury_high-precision_2009,
	title = {High-precision identification of contextual information in location-aware engineering applications},
	volume = {23},
	issn = {1474-0346},
	url = {http://www.sciencedirect.com/science/article/pii/S1474034609000184},
	doi = {10.1016/j.aei.2009.04.002},
	abstract = {This paper presents research that investigated algorithms for high-precision identification of contextual information in location-aware engineering applications. The primary contribution of the presented work is the design and implementation of a dynamic user-viewpoint tracking scheme in which mobile users’ spatial context is defined not only by their position (i.e., location), but also by their three-dimensional head orientation (i.e., line of sight). This allows the identification of objects and artifacts visible in a mobile user’s field of view with much higher accuracy than was possible by tracking position alone. For outdoor applications, a georeferencing based algorithm has been developed using the Global Positioning System ({GPS}) and magnetic orientation tracking devices [5] to track a user’s dynamic viewpoint. For indoor applications, this study explored the applicability of wireless technologies, in particular Indoor {GPS}, for dynamic user position tracking in situations where {GPS} is unavailable. The objectives of this paper are to describe the details of the three-stage-algorithm that has been designed and implemented, and to demonstrate the extent to which positioning technologies such as {GPS} and Indoor {GPS} can be used together with high-precision orientation trackers to accurately interpret the fully-qualified spatial context of a mobile user in challenging environments such as those found on construction sites. The obtained results highlight the potential of using location-aware technologies for rapidly identifying and retrieving contextual information in engineering applications.},
	number = {4},
	urldate = {2014-03-12},
	journal = {Advanced Engineering Informatics},
	author = {Khoury, Hiam M. and Kamat, Vineet R.},
	month = oct,
	year = {2009},
	pages = {483--496},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\J8UNQNKZ\\Khoury 그리고 Kamat - 2009 - High-precision identification of contextual inform.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\QETKPISS\\S1474034609000184.html:text/html}
}

@inproceedings{song_penlight:_2009,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '09},
	title = {{PenLight}: Combining a Mobile Projector and a Digital Pen for Dynamic Visual Overlay},
	isbn = {978-1-60558-246-7},
	shorttitle = {{PenLight}},
	url = {http://doi.acm.org/10.1145/1518701.1518726},
	doi = {10.1145/1518701.1518726},
	abstract = {Digital pen systems, originally designed to digitize annotations made on physical paper, are evolving to permit a wider variety of applications. Although the type and quality of pen feedback (e.g., haptic, audio, and visual) have a huge impact on advancing the digital pen technology, dynamic visual feedback has yet to be fully investigated. In parallel, miniature projectors are an emerging technology with the potential to enhance visual feedback for small mobile computing devices. In this paper we present the {PenLight} system, which is a testbed to explore the interaction design space and its accompanying interaction techniques in a digital pen embedded with a spatially-aware miniature projector. Using our prototype, that simulates a miniature projection (via a standard video projector), we visually augment paper documents, giving the user immediate access to additional information and computational tools. We also show how virtual ink can be managed in single and multi-user environments to aid collaboration and data management. User evaluation with professional architects indicated promise of our proposed techniques and their potential utility in the paper-intensive domain of architecture.},
	urldate = {2014-03-05},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Song, Hyunyoung and Grossman, Tovi and Fitzmaurice, George and Guimbretiere, Fran{\textbackslash}ccois and Khan, Azam and Attar, Ramtin and Kurtenbach, Gordon},
	year = {2009},
	keywords = {digital pen input, mobile projector, multi-layer interaction, spatially-aware display},
	pages = {143--152},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\R2Z8MCH8\\Song et al. - 2009 - PenLight Combining a Mobile Projector and a Digit.pdf:application/pdf}
}

@inproceedings{wright_open_1997,
	address = {Thessaloniki, Hellas},
	title = {Open Sound Control: A New Protocol for Communicating with Sound Synthesizers},
	url = {http://cnmat.berkeley.edu/publications/open_sound_control_new_protocol_communicating_sound_synthesizers},
	abstract = {Open {SoundControl} is a new protocol for communication among computers, sound synthesizers, and other multimedia devices that is optimized for modern networking technology. Entities within a system are addressed individually by an open-ended {URL}-style symbolic naming scheme that includes a powerful pattern matching language to specify multiple recipients of a single message. We provide high resolution time tags and a mechanism for specifying groups of messages whose effects are to occur simultaneously. There is also a mechanism for dynamically querying an Open {SoundControl} system to find out its capabilities and documentation of its features.},
	booktitle = {International Computer Music Conference},
	publisher = {International Computer Music Association},
	author = {Wright, Matthew and Freed, Adrian},
	year = {1997},
	keywords = {{OSC}},
	pages = {101--104}
}

@inproceedings{wellner_digitaldesk_1991,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '91},
	title = {The {DigitalDesk} Calculator: Tangible Manipulation on a Desk Top Display},
	isbn = {0-89791-451-1},
	shorttitle = {The {DigitalDesk} Calculator},
	url = {http://doi.acm.org/10.1145/120782.120785},
	doi = {10.1145/120782.120785},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 4th Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Wellner, Pierre},
	year = {1991},
	pages = {27--33},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\82BCQHI5\\Wellner - 1991 - The DigitalDesk Calculator Tangible Manipulation .pdf:application/pdf}
}

@article{davis_perceived_1989,
	title = {Perceived usefulness, perceived ease of use, and user acceptance of information technology},
	volume = {13},
	number = {3},
	journal = {{MIS} Quarterly},
	author = {Davis, FD},
	year = {1989},
	keywords = {bibtex-import},
	pages = {319--340}
}

@misc{perlman_user_2011,
	title = {User Interface Usability Evaluation with Web-Based Questionnaires},
	url = {http://oldwww.acm.org/perlman/question.html},
	urldate = {2014-03-17},
	journal = {acm.org},
	author = {Perlman, Gary},
	month = nov,
	year = {2011},
	note = {http://oldwww.acm.org/perlman/question.html},
	file = {User Interface Usability Evaluation with Web-Based Questionnaires:E\:\\Papers\\Zotero Repository\\storage\\4RJ53KPZ\\question.html:text/html}
}

@inproceedings{raskar_rfig_2004,
	address = {New York, {NY}, {USA}},
	series = {{SIGGRAPH} '04},
	title = {{RFIG} Lamps: Interacting with a Self-describing World via Photosensing Wireless Tags and Projectors},
	shorttitle = {{RFIG} Lamps},
	url = {http://doi.acm.org/10.1145/1186562.1015738},
	doi = {10.1145/1186562.1015738},
	abstract = {This paper describes how to instrument the physical world so that objects become self-describing, communicating their identity, geometry, and other information such as history or user annotation. The enabling technology is a wireless tag which acts as a radio frequency identity and geometry ({RFIG}) transponder. We show how addition of a photo-sensor to a wireless tag significantly extends its functionality to allow geometric operations - such as finding the 3D position of a tag, or detecting change in the shape of a tagged object. Tag data is presented to the user by direct projection using a handheld locale-aware mobile projector. We introduce a novel technique that we call interactive projection to allow a user to interact with projected information e.g. to navigate or update the projected information.The ideas are demonstrated using objects with active radio frequency ({RF}) tags. But the work was motivated by the advent of unpowered passive-{RFID}, a technology that promises to have significant impact in real-world applications. We discuss how our current prototypes could evolve to passive-{RFID} in the future.},
	urldate = {2014-03-12},
	booktitle = {{ACM} {SIGGRAPH} 2004 Papers},
	publisher = {{ACM}},
	author = {Raskar, Ramesh and Beardsley, Paul and van Baar, Jeroen and Wang, Yao and Dietz, Paul and Lee, Johnny and Leigh, Darren and Willwacher, Thomas},
	year = {2004},
	keywords = {augmented reality, human-machine communication, image stabilization, projector, radio frequency identification, stucture from motion},
	pages = {406--415},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\IKFUA6DB\\Raskar et al. - 2004 - RFIG Lamps Interacting with a Self-describing Wor.pdf:application/pdf}
}

@inproceedings{wimmer_curve:_2010,
	address = {New York, {NY}, {USA}},
	series = {{NordiCHI} '10},
	title = {Curve: Revisiting the Digital Desk},
	isbn = {978-1-60558-934-3},
	shorttitle = {Curve},
	url = {http://doi.acm.org/10.1145/1868914.1868977},
	doi = {10.1145/1868914.1868977},
	abstract = {Current desktop workspace environments consist of a vertical area (e.g., a screen with a virtual desktop) and a horizontal area (e.g., the physical desk). Daily working activities benefit from different intrinsic properties of both of these areas. However, both areas are distinct from each other, making data exchange between them cumbersome. Therefore, we present Curve, a novel interactive desktop environment, which combines advantages of vertical and horizontal working areas using a continous curved connection. This connection offers new ways of direct multi-touch interaction and new ways of information visualization. We describe our basic design, the ergonomic adaptions we made, and discuss technical challenges we met and expect to meet while building and configuring the system.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
	publisher = {{ACM}},
	author = {Wimmer, Raphael and Hennecke, Fabian and Schulz, Florian and Boring, Sebastian and Butz, Andreas and Hußmann, Heinrich},
	year = {2010},
	keywords = {curve, digital desks, direct-touch, ergonomics, interactive surfaces, tabletop interfaces, workplace},
	pages = {561--570},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\34S3XGP6\\Wimmer et al. - 2010 - Curve Revisiting the Digital Desk.pdf:application/pdf}
}

@misc{harrison_architects_design_????,
	title = {Design Process},
	url = {http://harrisonarchitects.com/tools/design_process},
	urldate = {2014-03-05},
	author = {HARRISON architects},
	note = {http://harrisonarchitects.com/tools/design\_process}
}

@inproceedings{pinhanez_creating_2003,
	address = {New York, {NY}, {USA}},
	series = {{MULTIMEDIA} '03},
	title = {Creating Touch-screens Anywhere with Interactive Projected Displays},
	isbn = {1-58113-722-2},
	url = {http://doi.acm.org/10.1145/957013.957112},
	doi = {10.1145/957013.957112},
	abstract = {We demonstrate a system that combines steerable projection and computer vision technologies to create "touch-screen" style interactive displays on any flat surface in a space. A high-end version of the system -- the Everywhere Display ({ED}) -- combines an {LCD} projector with motorized focus and zoom, a computer controlled pan-tilt mirror, and a pan-tilt zoom camera to enable steering of interactive projections around space. A low-end version ({ED}-lite) enables creation of interactive displays using a portable projector and camera attached to a laptop computer. Unlike traditional augmented reality systems, the {ED} systems enable delivery of interactive multimedia content on ordinary objects without requiring users to wear head mounted displays or carry special input devices.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the Eleventh {ACM} International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Pinhanez, Claudio and Kjeldsen, Rick and Tang, Lijun and Levas, Anthony and Podlaseck, Mark and Sukaviriya, Noi and Pingali, Gopal},
	year = {2003},
	keywords = {graphics, interfaces, smart spaces, steerable, ubiquitous, vision},
	pages = {460--461},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\9ZHGRJ5Z\\Pinhanez et al. - 2003 - Creating Touch-screens Anywhere with Interactive P.pdf:application/pdf}
}

@misc{_kj1/ofxprojectorkinectcalibration_????,
	title = {Kj1/{ofxProjectorKinectCalibration}},
	url = {https://github.com/Kj1/ofxProjectorKinectCalibration},
	abstract = {{ofxProjectorKinectCalibration} - Openframeworks addon for calibrating a kinect and a projector, currently work in progress},
	urldate = {2014-04-12},
	journal = {{GitHub}},
	file = {Snapshot:E\:\\Papers\\Zotero Repository\\storage\\6SJW5699\\ofxProjectorKinectCalibration.html:text/html}
}

@inproceedings{lopes_combining_2011,
	address = {New York, {NY}, {USA}},
	series = {{SBIM} '11},
	title = {Combining Bimanual Manipulation and Pen-based Input for 3D Modelling},
	isbn = {978-1-4503-0906-6},
	url = {http://doi.acm.org/10.1145/2021164.2021168},
	doi = {10.1145/2021164.2021168},
	abstract = {Multitouch enabled surfaces can bring advantages to modelling scenarios, in particular if bimanual and pen input can be combined. In this work, we assess the suitability of multitouch interfaces to 3D sketching tasks. We developed a multitouch enabled version of {ShapeShop}, whereby bimanual gestures allow users to explore the canvas through camera operations while using a pen to sketch. This provides a comfortable setting familiar to most users. Our contribution focuses on comparing the combined approach (bimanual and pen) to the pen-only interface for similar tasks. We conducted the evaluation helped by ten sketching experts who exercised both techniques. Results show that our approach both simplifies workflow and lowers task times, when compared to the pen-only interface, which is what most current sketching applications provide.},
	urldate = {2014-02-07},
	booktitle = {Proceedings of the Eighth Eurographics Symposium on Sketch-Based Interfaces and Modeling},
	publisher = {{ACM}},
	author = {Lopes, Pedro and Mendes, Daniel and Araújo, Bruno and Jorge, Joaquim A.},
	year = {2011},
	pages = {15--22},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\VNVQQF3Z\\Lopes et al. - 2011 - Combining Bimanual Manipulation and Pen-based Inpu.pdf:application/pdf}
}

@article{yeh_-site_2012,
	title = {On-site building information retrieval by using projection-based augmented reality},
	volume = {26},
	url = {http://ascelibrary.org/doi/abs/10.1061/(ASCE)CP.1943-5487.0000156},
	number = {3},
	urldate = {2014-03-05},
	journal = {Journal of Computing in Civil Engineering},
	author = {Yeh, Kai-Chen and Tsai, Meng-Han and Kang, Shih-Chung},
	year = {2012},
	pages = {342--355},
	file = {(ASCE)CP.1943-5487.pdf:E\:\\Papers\\Zotero Repository\\storage\\RNM2XD8G\\(ASCE)CP.1943-5487.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\HDPIXQB9\\cookieAbsent.html:text/html}
}

@article{wellner_interacting_1993,
	title = {Interacting with Paper on the {DigitalDesk}},
	volume = {36},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/159544.159630},
	doi = {10.1145/159544.159630},
	number = {7},
	urldate = {2014-03-12},
	journal = {Commun. {ACM}},
	author = {Wellner, Pierre},
	month = jul,
	year = {1993},
	pages = {87--96},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\S93BPHA2\\Wellner - 1993 - Interacting with Paper on the DigitalDesk.pdf:application/pdf}
}

@inproceedings{kim_ar_2014,
	title = {{AR} Lamp: interactions on projection-based augmented reality for interactive learning},
	shorttitle = {{AR} Lamp},
	url = {http://dl.acm.org/citation.cfm?id=2557505},
	urldate = {2014-03-05},
	booktitle = {Proceedings of the 19th international conference on Intelligent User Interfaces},
	publisher = {{ACM}},
	author = {Kim, Jeongyun and Seo, Jonghoon and Han, Tack-Don},
	year = {2014},
	pages = {353--358},
	file = {Snapshot:E\:\\Papers\\Zotero Repository\\storage\\38DNEZ6R\\citation.html:text/html}
}

@article{bae_high-precision_2013,
	title = {High-precision vision-based mobile augmented reality system for context-aware architectural, engineering, construction and facility management ({AEC}/{FM}) applications},
	volume = {1},
	issn = {2213-7459},
	url = {http://link.springer.com/article/10.1186/2213-7459-1-3},
	doi = {10.1186/2213-7459-1-3},
	abstract = {Background Many context-aware techniques have been proposed to deliver cyber-information, such as project specifications or drawings, to on-site users by intelligently interpreting their environment. However, these techniques primarily rely on {RF}-based location tracking technologies (e.g., {GPS} or {WLAN}), which typically do not provide sufficient precision in congested construction sites or require additional hardware and custom mobile devices. Method This paper presents a new vision-based mobile augmented reality system that allows field personnel to query and access 3D cyber-information on-site by using photographs taken from standard mobile devices. The system does not require any location tracking modules, external hardware attachments, and/or optical fiducial markers for localizing a user’s position. Rather, the user’s location and orientation are purely derived by comparing images from the user’s mobile device to a 3D point cloud model generated from a set of pre-collected site photographs. Results The experimental results show that 1) the underlying 3D reconstruction module of the system generates complete 3D point cloud models of target scene, and is up to 35 times faster than other state-of-the-art Structure-from-Motion ({SfM}) algorithms, 2) the localization time takes at most few seconds in actual construction site. Conclusion The localization speed and empirical accuracy of the system provides the ability to use the system on real-world construction sites. Using an actual construction case study, the perceived benefits and limitations of the proposed method for on-site context-aware applications are discussed in detail.},
	language = {en},
	number = {1},
	urldate = {2014-03-08},
	journal = {Visualization in Engineering},
	author = {Bae, Hyojoon and Golparvar-Fard, Mani and White, Jules},
	month = dec,
	year = {2013},
	keywords = {Computer-Aided Engineering ({CAD}, {CAE}) and Design, Engineering Design, Industrial Design},
	pages = {1--13},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\Z5VG42I5\\Bae et al. - 2013 - High-precision vision-based mobile augmented reali.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\9D25DT5V\\2213-7459-1-3.html:text/html}
}

@inproceedings{kato_marker_1999,
	title = {Marker tracking and {HMD} calibration for a video-based augmented reality conferencing system},
	doi = {10.1109/IWAR.1999.803809},
	abstract = {We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on virtual monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and head mounted display ({HMD}) calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through {HMD} based on the marker tracking},
	booktitle = {2nd {IEEE} and {ACM} International Workshop on Augmented Reality, 1999. ({IWAR} '99) Proceedings},
	author = {Kato, H. and Billinghurst, M.},
	year = {1999},
	keywords = {3D {CSCW}, augmented reality, calibration, calibration method, collaboration, collaborative work, Computer displays, Computer interfaces, computer vision, fiducial markers, groupware, head mounted display, head-up displays, {HMD} calibration, Humans, image registration, marker tracking, optical see-through {HMD}, overlay, precise virtual image registration, real world, remote collaborators, shared virtual whiteboard, teleconferencing, video-based augmented reality conferencing system, video signal processing, Virtual environment, virtual images, virtual monitors, Virtual reality},
	pages = {85--94},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\CZDME9CJ\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\NGFAT654\\Kato 그리고 Billinghurst - 1999 - Marker tracking and HMD calibration for a video-ba.pdf:application/pdf}
}

@article{guiard_asymmetric_1987,
	title = {Asymmetric Division of Labor in Human Skilled Bimanual Action},
	volume = {19},
	issn = {0022-2895},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00222895.1987.10735426},
	doi = {10.1080/00222895.1987.10735426},
	abstract = {This article presents a tentative theoretical framework for the study of asymmetry in the context of human bimanual action. It is emphasized that in man most skilled manual activities involve two hands playing different roles, a fact that has been often overlooked in the experimental study of human manual lateralization. As an alternative to the current concepts of manual preference and manual superiority—whose relevance is limited to the particular case of unimanual actions—the more general concept of lateral preference is proposed, to denote preference for one of the two possible ways of assigning two roles to two hands. A simple model describing man’s favored intermanual division of labor in the variety of his skilled manual activities is outlined. The two main assumptions of the model are the following. 1) The two hands represent two motors, that is, devices serving to create motion, whose internal (biomechanical and physiological) complexity is ignored in the suggested approach. 2) In man, the two manual motors cooperate with one another as if they were assembled in series, thereby forming a kinematic chain: In a right-hander allowed to follow his or her lateral preferences, motion produced by the right hand tends to articulate with motion produced by the left. It is suggested that the kinematic chain model may help in understanding the adaptive advantage of human manual specialization.},
	number = {4},
	urldate = {2014-03-07},
	journal = {Journal of Motor Behavior},
	author = {Guiard, Yves},
	year = {1987},
	pmid = {15136274},
	pages = {486--517},
	file = {Snapshot:E\:\\Papers\\Zotero Repository\\storage\\GK6XET56\\00222895.1987.html:text/html}
}

@inproceedings{zeleznik_sketch:_2007,
	address = {New York, {NY}, {USA}},
	series = {{SIGGRAPH} '07},
	title = {{SKETCH}: An Interface for Sketching 3D Scenes},
	isbn = {978-1-4503-1823-5},
	shorttitle = {{SKETCH}},
	url = {http://doi.acm.org/10.1145/1281500.1281530},
	doi = {10.1145/1281500.1281530},
	abstract = {Sketching communicates ideas rapidly through approximate visual images with low overhead (pencil and paper), no need for precision or specialized knowledge, and ease of low-level correction and revision. In contrast, most 3D computer modeling systems are good at generating arbitrary views of precise 3D models and support high-level editing and revision. The {SKETCH} application described in this paper attempts to combine the advantages of each in order to create an environment for rapidly conceptualizing and editing approximate 3D scenes. To achieve this, {SKETCH} uses simple non-photorealistic rendering and a purely gestural interface based on simplified line drawings of primitives that allows all operations to be specified within the 3D world.},
	urldate = {2014-02-07},
	booktitle = {{ACM} {SIGGRAPH} 2007 Courses},
	publisher = {{ACM}},
	author = {Zeleznik, Robert C. and Herndon, Kenneth P. and Hughes, John F.},
	year = {2007},
	keywords = {3D modeling, direct manipulation, gestural interface, interaction techniques, nonphotorealistic rendering, sketching},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\C9FNEMIU\\Zeleznik et al. - 2007 - SKETCH An Interface for Sketching 3D Scenes.pdf:application/pdf}
}

@incollection{pinhanez_everywhere_2001,
	series = {Lecture Notes in Computer Science},
	title = {The Everywhere Displays Projector: A Device to Create Ubiquitous Graphical Interfaces},
	copyright = {©2001 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-42614-1, 978-3-540-45427-4},
	shorttitle = {The Everywhere Displays Projector},
	url = {http://link.springer.com/chapter/10.1007/3-540-45427-6_27},
	abstract = {This paper introduces the Everywhere Displays projector, a device that uses a rotating mirror to steer the light from an {LCD}/{DLP} projector onto different surfaces of an environment. Issues of brightness, oblique projection distortion, focus, obstruction, and display resolution are examined. Solutions to some of these problems are described, together with a plan to use a video camera to allow device-free interaction with the projected images. The {ED}-projector is a practical way to create ubiquitous graphical interfaces to access computational power and networked data. In particular, it is envisioned as an alternative to the carrying of laptops and to the installation of displays in furniture, objects, and walls. In addition, the use of {ED}-projectors to augment reality without the use of goggles is examined and illustrated with examples.},
	language = {en},
	number = {2201},
	urldate = {2014-03-12},
	booktitle = {Ubicomp 2001: Ubiquitous Computing},
	publisher = {Springer Berlin Heidelberg},
	author = {Pinhanez, Claudio},
	editor = {Abowd, Gregory D. and Brumitt, Barry and Shafer, Steven},
	month = jan,
	year = {2001},
	keywords = {Computer Communication Networks, Computers and Society, Information Systems Applications (incl.Internet), Operating Systems, User Interfaces and Human Computer Interaction},
	pages = {315--331},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\7H2ISKHE\\Pinhanez - 2001 - The Everywhere Displays Projector A Device to Cre.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\HUUK7CRT\\3-540-45427-6_27.html:text/html}
}

@article{xue_it_2012,
	title = {{IT} supported collaborative work in A/E/C projects: A ten-year review},
	volume = {21},
	issn = {0926-5805},
	shorttitle = {{IT} supported collaborative work in A/E/C projects},
	url = {http://www.sciencedirect.com/science/article/pii/S0926580511001002},
	doi = {10.1016/j.autcon.2011.05.016},
	abstract = {The strategic implementation of innovative, collaborative and integrated information is essential for the sustainable development of construction organizations in the new century. The emerging information technologies ({ITs}) offer construction organizations great potential to develop collaborative work ({CW}) management information systems in architecture, engineering and construction (A/E/C) projects. This paper presents an in-depth literature review of {IT} supported {CW} in A/E/C projects. Based on selected papers from well-known academic journals in construction management over a ten-year period (2000–2009), the review focuses on the implementations of {IT} in collaborative design, collaborative construction project management, and integrated inter-organization management information systems. A synthesis is presented on the state-of-the-art and trends of {IT} supported {CW} in A/E/C projects. Some limitations of past research studies in {IT} supported {CW} in A/E/C projects are discussed and future research directions are recommended.},
	urldate = {2014-03-05},
	journal = {Automation in Construction},
	author = {Xue, Xiaolong and Shen, Qiping and Fan, Hongqin and Li, Heng and Fan, Shichao},
	month = jan,
	year = {2012},
	keywords = {collaborative work, Construction engineering management, Information technology, Literature review, Project management},
	pages = {1--9},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\S2Q2VWKZ\\Xue et al. - 2012 - IT supported collaborative work in AEC projects.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\AAPG644C\\S0926580511001002.html:text/html}
}

@article{koike_integrating_2001,
	title = {Integrating Paper and Digital Information on {EnhancedDesk}: A Method for Realtime Finger Tracking on an Augmented Desk System},
	volume = {8},
	issn = {1073-0516},
	shorttitle = {Integrating Paper and Digital Information on {EnhancedDesk}},
	url = {http://doi.acm.org/10.1145/504704.504706},
	doi = {10.1145/504704.504706},
	abstract = {This article describes a design and implementation of an augmented desk system, named {EnhancedDesk}, which smoothly integrates paper and digital information on a desk. The system provides users an intelligent environment that automatically retrieves and displays digital information corresponding to the real objects (e.g., books) on the desk by using computer vision. The system also provides users direct manipulation of digital information by using the users' own hands and fingers for more natural and more intuitive interaction. Based on the experiments with our first prototype system, some critical issues on augmented desk systems were identified when trying to pursue rapid and fine recognition of hands and fingers. To overcome these issues, we developed a novel method for realtime finger tracking on an augmented desk system by introducing a infrared camera, pattern matching with normalized correlation, and a pan-tilt camera. We then show an interface prototype on {EnhancedDesk}. It is an application to a computer-supported learning environment, named Interactive Textbook. The system shows how effective the integration of paper and digital information is and how natural and intuitive direct manipulation of digital information with users' hands and fingers is.},
	number = {4},
	urldate = {2014-03-12},
	journal = {{ACM} Trans. Comput.-Hum. Interact.},
	author = {Koike, Hideki and Sato, Yoichi and Kobayashi, Yoshinori},
	month = dec,
	year = {2001},
	keywords = {augmented reality, computer-supported learning, computer vision, education, finger/hand recognition, infrared camera, perceptive user interfaces},
	pages = {307--322},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\NNRQMZKV\\2001 - Integrating Paper and Digital Information on Enhan.pdf:application/pdf}
}

@inproceedings{kim_ilight:_2010,
	address = {New York, {NY}, {USA}},
	series = {{CHI} {EA} '10},
	title = {{iLight}: Information Flashlight on Objects Using Handheld Projector},
	isbn = {978-1-60558-930-5},
	shorttitle = {{iLight}},
	url = {http://doi.acm.org/10.1145/1753846.1754030},
	doi = {10.1145/1753846.1754030},
	urldate = {2014-03-12},
	booktitle = {{CHI} '10 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Kim, Sunjun and Chung, Jaewoo and Oh, Alice and Schmandt, Chris and Kim, Ig-Jae},
	year = {2010},
	keywords = {augmented reality, handheld projector, interactive object, object augmentation},
	pages = {3631--3636},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\5KCNU9RT\\Kim et al. - 2010 - iLight Information Flashlight on Objects Using Ha.pdf:application/pdf}
}

@article{chen_framework_2011,
	title = {A framework for using mobile computing for information management on construction sites},
	volume = {20},
	issn = {0926-5805},
	url = {http://www.sciencedirect.com/science/article/pii/S0926580511000033},
	doi = {10.1016/j.autcon.2011.01.002},
	abstract = {The application of mobile computing in construction is becoming a major research theme in the domain of Information Technology in Construction. However, most research in this area focuses on a detailed aspect or single facet of a mobile computing technology. This paper introduces a framework for the implementation of mobile computing on construction sites, which comprises an application model and a technical model. The application model identifies the features of mobile computing, construction personnel, construction information, and construction sites, and explores the interactions that are likely to affect the implementation of mobile computing. The technological model generalizes mobile computing technologies and gives system designers a clear structure for designing mobile computing systems from a technical perspective. Finally, a case study of a real construction situation is used to validate this framework.},
	number = {7},
	urldate = {2014-03-12},
	journal = {Automation in Construction},
	author = {Chen, Yuan and Kamara, John M.},
	month = nov,
	year = {2011},
	keywords = {Construction information management, Construction sites, mobile computing},
	pages = {776--788},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\WSK7N58U\\Chen 그리고 Kamara - 2011 - A framework for using mobile computing for informa.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\ABX5N224\\S0926580511000033.html:text/html}
}

@misc{wikipedia_contributors_wimp_2014,
	title = {{WIMP} (computing)},
	copyright = {Creative Commons Attribution-{ShareAlike} License},
	url = {http://en.wikipedia.org/w/index.php?title=WIMP_(computing)&oldid=593323144},
	abstract = {In human–computer interaction, {WIMP} stands for "windows, icons, menus, pointer",[1][2][3] denoting a style of interaction using these elements of the user interface. It was coined by Merzouga Wilberts in 1980.[4] Other expansions are sometimes used, substituting "mouse" and "mice" or "pull-down menu" and "pointing", for menus and pointer, respectively.[5][6][7]},
	language = {en},
	urldate = {2014-03-06},
	journal = {Wikipedia, the free encyclopedia},
	author = {Wikipedia contributors},
	month = mar,
	year = {2014},
	note = {Page Version {ID}: 593323144},
	file = {Snapshot:E\:\\Papers\\Zotero Repository\\storage\\QPW37TDN\\index.html:text/html}
}

@inproceedings{sukaviriya_portable_2004,
	address = {New York, {NY}, {USA}},
	series = {{CHI} {EA} '04},
	title = {A Portable System for Anywhere Interactions},
	isbn = {1-58113-703-6},
	url = {http://doi.acm.org/10.1145/985921.985937},
	doi = {10.1145/985921.985937},
	abstract = {Interactions have taken off from the confinement of a single screen into various personal devices. Projected an interface onto different parts of a physical environment is an escape beyond traditional display devices. Imagine that any walls or floors can turn into a direct manipulation space without a lot of effort. This demonstration of {ED}-lite, a combination of a laptop, custom software, off-the-shelf digital camera and projector, shows projected interfaces with interactions on any surfaces including those not necessarily perpendicular to the projector. {ED}-lite is a derivation of our previous work on Everywhere Displays ({ED}) and steerable interfaces. This portable version has an automatic calibration feature that makes applications usable on any surfaces in a drop. More importantly, it is now possible to be taken on the road for demonstrations.},
	urldate = {2014-03-12},
	booktitle = {{CHI} '04 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Sukaviriya, Noi and Kjeldsen, Rick and Pinhanez, Claudio and Tang, Lijun and Levas, Anthony and Pingali, Gopal and Podlaseck, Mark},
	year = {2004},
	keywords = {augmented reality, interactive spaces, projected interfaces vision-based interactions, ubiquitous computing},
	pages = {789--790},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\SHD5ZCWA\\Sukaviriya et al. - 2004 - A Portable System for Anywhere Interactions.pdf:application/pdf}
}

@inproceedings{song_mouselight:_2010,
	title = {{MouseLight}: bimanual interactions on digital paper using a pen and a spatially-aware mobile projector},
	shorttitle = {{MouseLight}},
	url = {http://dl.acm.org/citation.cfm?id=1753697},
	urldate = {2014-03-05},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Song, Hyunyoung and Guimbretiere, Francois and Grossman, Tovi and Fitzmaurice, George},
	year = {2010},
	pages = {2451--2460},
	file = {[PDF] from autodeskresearch.com:E\:\\Papers\\Zotero Repository\\storage\\J297985H\\Song et al. - 2010 - MouseLight bimanual interactions on digital paper.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\8R3CSP62\\citation.html:text/html}
}

@misc{opencv_2.4.8.0_documentation_camera_????,
	title = {Camera calibration With {OpenCV}},
	url = {http://docs.opencv.org/doc/tutorials/calib3d/camera_\\calibration/camera_calibration.html},
	urldate = {2014-04-12},
	journal = {{OpenCV} 2.4.8.0 documentation},
	author = {OpenCV 2.4.8.0 documentation},
	note = {http://docs.opencv.org/doc/tutorials/calib3d/camera\_\\calibration/camera\_calibration.html},
	file = {Camera calibration With OpenCV — OpenCV 2.4.8.0 documentation:E\:\\Papers\\Zotero Repository\\storage\\6RU499P7\\camera_calibration.html:text/html}
}

@incollection{talmaki_geospatial_2010,
	title = {Geospatial Databases and Augmented Reality Visualization for Improving Safety in Urban Excavation Operations},
	isbn = {978-0-7844-1109-4},
	url = {http://ascelibrary.org/doi/abs/10.1061/41109%28373%2910},
	abstract = {The U.S. has more than 14 million miles of buried pipelines and utilities, many of which are in congested urban environments where several lines share the underground space. Errors in locating excavations for new installation or for repair/rehabilitation of existing utilities can result in significant costs, delays, loss of life, and damage to property (Sterling 2000). There is thus a clear need for new solutions to accurately locate buried infrastructure and improve excavation safety. This paper presents ongoing research being collaboratively conducted by the University of Michigan and {DTE} Energy (Michigan's largest electric and gas utility company) that is investigating the use of Real-Time Kinematic {GPS}, combined with Geospatial Databases of subsurface utilities to design a new visual excavator-utility collision avoidance technology. 3D models of buried utilities are created from available geospatial data, and then superimposed over an excavator's work space using geo-referenced Augmented Reality ({AR}) to provide the operator and the spotter(s) with visual information on the location and type of utilities that exist in the excavator's vicinity. This paper describes the overall methodology and the first results of the research.},
	urldate = {2014-03-06},
	booktitle = {Construction Research Congress 2010},
	publisher = {American Society of Civil Engineers},
	author = {Talmaki, S. and Dong, S. and Kamat, V.},
	year = {2010},
	pages = {91--101},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\T9G3P3IX\\Talmaki et al. - Geospatial Databases and Augmented Reality Visuali.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\NDVWUE6U\\41109(373)10.html:text/html}
}

@article{song_modelcraft_2009,
	title = {The {ModelCraft} framework: Capturing freehand annotations and edits to facilitate the 3D model design process using a digital pen},
	volume = {16},
	shorttitle = {The {ModelCraft} framework},
	url = {http://creativemachines.cornell.edu/sites/default/files/TCHI09_Song.pdf},
	number = {14},
	urldate = {2014-02-13},
	journal = {{ACM} Transactions on Computer-Human Interaction},
	author = {Song, Hyunyoung and Guimbretière, Fran{\textbackslash}ccois and Lipson, Hod},
	year = {2009},
	pages = {1--14},
	file = {[PDF] from cornell.edu:E\:\\Papers\\Zotero Repository\\storage\\5R577AEW\\Song et al. - 2009 - The ModelCraft framework Capturing freehand annot.pdf:application/pdf}
}

@article{irizarry_infospot:_2013,
	title = {{InfoSPOT}: A mobile Augmented Reality method for accessing building information through a situation awareness approach},
	volume = {33},
	issn = {0926-5805},
	shorttitle = {{InfoSPOT}},
	url = {http://www.sciencedirect.com/science/article/pii/S0926580512001513},
	doi = {10.1016/j.autcon.2012.09.002},
	abstract = {The Architecture, Engineering, Construction, and Owner/Operator ({AECO}) industry is constantly searching for new methods for increasing efficiency and productivity. Facility Managers ({FMs}), as a part of the owner/operator role, work in complex and dynamic environments where critical decisions are constantly made. This decision-making process and its consequent performance can be improved by enhancing Situation Awareness ({SA}) of the {FMs} through new digital technologies. In this paper, {InfoSPOT} (Information Surveyed Point for Observation and Tracking), is recommended to {FMs} as a mobile Augmented Reality ({AR}) tool for accessing information about the facilities they maintain. {AR} has been considered as a viable option to reduce inefficiencies of data overload by providing {FMs} with a {SA}-based tool for visualizing their “real-world” environment with added interactive data. A prototype of the {AR} application was developed and a user participation experiment and analysis conducted to evaluate the features of {InfoSPOT}. This innovative application of {AR} has the potential to improve construction practices, and in this case, facility management.},
	urldate = {2014-03-11},
	journal = {Automation in Construction},
	author = {Irizarry, Javier and Gheisari, Masoud and Williams, Graceline and Walker, Bruce N.},
	month = aug,
	year = {2013},
	keywords = {augmented reality, Facility management, Situation awareness},
	pages = {11--23},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\6BPM3E6G\\Irizarry et al. - 2013 - InfoSPOT A mobile Augmented Reality method for ac.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\6FQBJCRJ\\S0926580512001513.html:text/html}
}

@article{behzadan_enabling_2013,
	title = {Enabling discovery‐based learning in construction using telepresent augmented reality},
	volume = {33},
	issn = {0926-5805},
	url = {http://www.sciencedirect.com/science/article/pii/S0926580512001525},
	doi = {10.1016/j.autcon.2012.09.003},
	abstract = {Construction engineering students often complain about the lack of engagement and interaction with the learning environment. Notwithstanding, many instructors still rely on traditional teaching methods which include the use of chalkboard, handouts, and computer presentations that are often filled with many words and few visual elements. Research shows that these teaching techniques are considered almost obsolete by a many students specially those who are visual learners or team workers. Also, the influence of visual and social media has changed student perceptions and how they expect the instructional materials to be presented in a classroom setting. This paper presents an innovative pedagogical tool that uses remote videotaping, augmented reality ({AR}), and ultra-wide band ({UWB}) locationing to bring live videos of remote construction jobsites to the classroom, create an intuitive interface for students to interact with the objects in the video scenes, and visually deliver location-aware instructional materials to them.},
	urldate = {2014-03-12},
	journal = {Automation in Construction},
	author = {Behzadan, Amir H. and Kamat, Vineet R.},
	month = aug,
	year = {2013},
	keywords = {augmented reality, Construction engineering, education, Learning, Visualization},
	pages = {3--10},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\4ZBSBD6H\\Behzadan 그리고 Kamat - 2013 - Enabling discovery‐based learning in construction .pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\FG5HNSZ6\\S0926580512001525.html:text/html}
}

@article{giretti_design_2009,
	title = {Design and first development of an automated real‐time safety management system for construction sites},
	volume = {15},
	issn = {1392-3730},
	url = {http://www.tandfonline.com/doi/abs/10.3846/1392-3730.2009.15.325-336},
	doi = {10.3846/1392-3730.2009.15.325-336},
	abstract = {Abstract This paper reports a feasibility study which addressed the development of a new, advanced system mainly devoted to automatic real‐time health and safety management on construction sites. The preliminary analyses and experiments described in this paper concern two of the most important functionalities which must be included in the system's final release. The first functionality consists in real‐time position‐tracking of workers involved on construction sites and the second ‐ in a software tool for the prevention of non‐authorized access to dangerous zones. This research step is part of a vaster, ongoing research project, addressing the development of a new generation of advanced construction management systems which allow real‐time monitoring and coordination of tasks, automatic health and safety management, on‐site delivery of technical information and the capture of “as‐built” documentation. This paper focuses mainly on the development of a reliable methodology for real‐time monitoring of the position of both workers and equipment in outdoor construction sites by applying Ultra Wide Band ({UWB}) based technologies. This positioning system was then interfaced with a software tool which performs virtual fencing of pre‐selected, dangerous areas. Guidelines for the design of the receivers’ topology will be addressed and the results of measurements recorded on a typical medium‐sized block of flats, during different phases of the construction progress will be summed up. Finally, the preliminary experimental results obtained by the virtual fencing application tool will be presented and used to plan future research objectives.},
	number = {4},
	urldate = {2014-03-12},
	journal = {Journal of Civil Engineering and Management},
	author = {Giretti, Alberto and Carbonari, Alessandro and Naticchia, Berardo and DeGrassi, Mario},
	year = {2009},
	pages = {325--336},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\GCEPWBTK\\Giretti et al. - 2009 - Design and first development of an automated real‐.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\FF8PNFFV\\1392-3730.2009.15.html:text/html}
}

@article{chi_research_2013,
	title = {Research trends and opportunities of augmented reality applications in architecture, engineering, and construction},
	volume = {33},
	issn = {0926-5805},
	url = {http://www.sciencedirect.com/science/article/pii/S0926580513000022},
	doi = {10.1016/j.autcon.2012.12.017},
	abstract = {Augmented reality ({AR}), a state-of-the-art technology for superimposing information onto the real world, has recently started to affect our daily lives. {AR} applications are becoming mature and versatile. This paper discusses trends in {AR} applications for architecture, engineering, construction, and facility management ({AEC}/{FM}). This paper specifically focuses on four technologies—localization, natural user interface ({NUI}), cloud computing, and mobile devices—which have the potential to influence the development of {AR} applications. Advances in localization technology will enable the deployment of {AR} in a complex environment. An {NUI} provides more convenient and intuitive user experiences, which can increase the usability of {AR}. Cloud computing environments allow users with internet access to ubiquitously retrieve information from almost anywhere. Hence, cloud computing increases the freedom of using {AR} in {AEC}/{FM} applications. Another factor that will lead to the wider usage of {AR} is that mobile devices are becoming smaller, more powerful, and less expensive. This paper summarizes the results of 101 research efforts, and outlines the research trends and opportunities for applying {AR} in the fields of {AEC}/{FM}.},
	urldate = {2014-03-06},
	journal = {Automation in Construction},
	author = {Chi, Hung-Lin and Kang, Shih-Chung and Wang, Xiangyu},
	month = aug,
	year = {2013},
	keywords = {{AEC}/{FM}, augmented reality, Cloud computing environment, Localization, Natural user interface, Portability},
	pages = {116--122},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\8VCIGCJP\\Chi et al. - 2013 - Research trends and opportunities of augmented rea.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\FD2EDPM6\\S0926580513000022.html:text/html}
}

@inproceedings{rekimoto_augmented_1999,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '99},
	title = {Augmented Surfaces: A Spatially Continuous Work Space for Hybrid Computing Environments},
	isbn = {0-201-48559-1},
	shorttitle = {Augmented Surfaces},
	url = {http://doi.acm.org/10.1145/302979.303113},
	doi = {10.1145/302979.303113},
	abstract = {This paper describes our design and implementation of a computer
augmented environment that allows users to smoothly interchange
digital information among their portable computers, table and wall
displays, and other physical objects. Supported by a camera-based
object recognition system, users can easily integrate their
portable computers with the pre-installed ones in the environment.
Users can use displays projected on tables and walls as a spatially
continuous extension of their portable computers. Using an
interaction technique called hyperdragging, users can transfer
information from one computer to another, by only knowing the
physical relationship between them. We also provide a mechanism for
attaching digital data to physical objects, such as a videotape or
a document folder, to link physical and digital spaces.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Rekimoto, Jun and Saitoh, Masanori},
	year = {1999},
	keywords = {architectural media, augmented reality, multiple device user interfaces, physical space, portable computers, table-sized displays, ubiquitous computing, wall-sized displays},
	pages = {378--385},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\NJV6J7FZ\\Rekimoto 그리고 Saitoh - 1999 - Augmented Surfaces A Spatially Continuous Work Sp.pdf:application/pdf}
}

@misc{autodesk_3ds_????,
	title = {3ds Max},
	url = {http://www.autodesk.com/products/autodesk-3ds-max/overview},
	urldate = {2014-03-06},
	author = {Autodesk},
	note = {http://www.autodesk.com/products/autodesk-3ds-max/overview},
	file = {3ds Max | 3D Modeling and Rendering Software | Autodesk:E\:\\Papers\\Zotero Repository\\storage\\KJSCRNE5\\overview.html:text/html}
}

@article{dong_smart:_2013,
	title = {{SMART}: scalable and modular augmented reality template for rapid development of engineering visualization applications},
	volume = {1},
	copyright = {2013 Dong and Kamat; licensee Springer.},
	issn = {2213-7459},
	shorttitle = {{SMART}},
	url = {http://www.viejournal.com/content/1/1/1/},
	doi = {10.1186/2213-7459-1-1},
	abstract = {The visualization of civil infrastructure systems and processes is critical for the validation and communication of computer generated models to decision-makers. Augmented Reality ({AR}) visualization blends real-world information with graphical 3D models to create informative composite views that are difficult to create or replicate on the computer alone.},
	language = {en},
	number = {1},
	urldate = {2014-03-08},
	journal = {Visualization in Engineering},
	author = {Dong, Suyang and Kamat, Vineet R.},
	month = jun,
	year = {2013},
	pages = {1},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\XMKZMPZM\\Dong 그리고 Kamat - 2013 - SMART scalable and modular augmented reality temp.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\BWQEQKNR\\1.html:text/html}
}

@inproceedings{igarashi_teddy:_2007,
	address = {New York, {NY}, {USA}},
	series = {{SIGGRAPH} '07},
	title = {Teddy: A Sketching Interface for 3D Freeform Design},
	isbn = {978-1-4503-1823-5},
	shorttitle = {Teddy},
	url = {http://doi.acm.org/10.1145/1281500.1281532},
	doi = {10.1145/1281500.1281532},
	abstract = {We present a sketching interface for quickly and easily designing freeform models such as stuffed animals and other rotund objects. The user draws several 2D freeform strokes interactively on the screen and the system automatically constructs plausible 3D polygonal surfaces. Our system supports several modeling operations, including the operation to construct a 3D polygonal surface from a 2D silhouette drawn by the user: it inflates the region surrounded by the silhouette making wide areas fat, and narrow areas thin. Teddy, our prototype system, is implemented as a Java™ program, and the mesh construction is done in real-time on a standard {PC}. Our informal user study showed that a first-time user typically masters the operations within 10 minutes, and can construct interesting 3D models within minutes.},
	urldate = {2014-03-05},
	booktitle = {{ACM} {SIGGRAPH} 2007 Courses},
	publisher = {{ACM}},
	author = {Igarashi, Takeo and Matsuoka, Satoshi and Tanaka, Hidehiko},
	year = {2007},
	keywords = {3D modeling, chordal axes, design, gestures, inflation, pen-based systems, sketching},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\FD7AXBXE\\Igarashi et al. - 2007 - Teddy A Sketching Interface for 3D Freeform Desig.pdf:application/pdf}
}

@inproceedings{coram_astrotouch:_2013,
	address = {New York, {NY}, {USA}},
	series = {{ITS} '13},
	title = {{AstroTouch}: A Multi-touch Digital Desktop for Astrodynamics},
	isbn = {978-1-4503-2271-3},
	shorttitle = {{AstroTouch}},
	url = {http://doi.acm.org/10.1145/2512349.2512793},
	doi = {10.1145/2512349.2512793},
	abstract = {In this paper, we present the design, implementation, and preliminary evaluation of {AstroTouch}, a prototype desktop surface application to support analysis and visualization in the field of astrodynamics. We describe the fundamental characteristics of this complex scientific domain and discuss how these characteristics, combined with an assessment of current research surrounding multi-touch and the digital desktop, informed the design of our system. We detail the prototype implementation and present the results of an initial design critique conducted with domain experts.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 2013 {ACM} International Conference on Interactive Tabletops and Surfaces},
	publisher = {{ACM}},
	author = {Coram, Jamie L. and Iverson, Rob and Ackerman, Andrew},
	year = {2013},
	keywords = {astrodynamics, digital desktop, ergonomics, interactive surfaces, multi-touch, orbital maneuvers},
	pages = {11--14},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\9BQI5C3P\\Coram et al. - 2013 - AstroTouch A Multi-touch Digital Desktop for Astr.pdf:application/pdf}
}

@inproceedings{kane_bonfire:_2009,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '09},
	title = {Bonfire: A Nomadic System for Hybrid Laptop-tabletop Interaction},
	isbn = {978-1-60558-745-5},
	shorttitle = {Bonfire},
	url = {http://doi.acm.org/10.1145/1622176.1622202},
	doi = {10.1145/1622176.1622202},
	abstract = {We present Bonfire, a self-contained mobile computing system that uses two laptop-mounted laser micro-projectors to project an interactive display space to either side of a laptop keyboard. Coupled with each micro-projector is a camera to enable hand gesture tracking, object recognition, and information transfer within the projected space. Thus, Bonfire is neither a pure laptop system nor a pure tabletop system, but an integration of the two into one new nomadic computing platform. This integration (1) enables observing the periphery and responding appropriately, e.g., to the casual placement of objects within its field of view, (2) enables integration between physical and digital objects via computer vision, (3) provides a horizontal surface in tandem with the usual vertical laptop display, allowing direct pointing and gestures, and (4) enlarges the input/output space to enrich existing applications. We describe Bonfire's architecture, and offer scenarios that highlight Bonfire's advantages. We also include lessons learned and insights for further development and use.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 22Nd Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Kane, Shaun K. and Avrahami, Daniel and Wobbrock, Jacob O. and Harrison, Beverly and Rea, Adam D. and Philipose, Matthai and LaMarca, Anthony},
	year = {2009},
	keywords = {ambient interaction, computer vision, extended display, gestures, laptop, micro-projector, object recognition, peripheral display, surface, tabletop, tangible bits},
	pages = {129--138},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\CKVWF2NR\\Kane et al. - 2009 - Bonfire A Nomadic System for Hybrid Laptop-tablet.pdf:application/pdf}
}

@article{aziz_supporting_2012,
	title = {Supporting Site-Based Processes Using Context-Aware Virtual Prototyping},
	volume = {18},
	issn = {1076-0431},
	url = {http://ascelibrary.org/doi/abs/10.1061/%28ASCE%29AE.1943-5568.0000068},
	doi = {10.1061/(ASCE)AE.1943-5568.0000068},
	abstract = {Abstract Recent advances in the use of virtual building prototypes as a tool to support various site-based production processes coupled with improvements in context-aware mobile computing offer significant potential of improving decision making during construction phase of a building by providing relevant and timely access to design and process information. This paper investigates the scope and potential for integrating intelligent context-aware interfaces with sophisticated virtual building prototypes, to provide highly relevant and context-specific building information to concerned stakeholders throughout the building life cycle. Key enabling technologies and related literature are reviewed. Architecture to integrate context awareness with virtual building prototype is presented to allow for integration of context at various levels, and timely retrieval of building model data is presented. Conclusions are drawn about the possible future effect on the construction industry.},
	number = {2},
	urldate = {2014-03-12},
	journal = {Journal of Architectural Engineering},
	author = {Aziz, Z.},
	year = {2012},
	pages = {79--83},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\9VVVMXMK\\Aziz - 2012 - Supporting Site-Based Processes Using Context-Awar.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\H3QKPBID\\(ASCE)AE.1943-5568.html:text/html}
}

@techreport{yu_prototype_2007,
	address = {Simon Fraser University},
	title = {A Prototype Sketch-Based Architectural Design System with Behavior Mode},
	institution = {School of Computing Science,},
	author = {Yu, Jun and Zhang, Hao},
	year = {2007},
	pages = {1--7}
}

@incollection{grossman__2010,
	series = {Human-Computer Interaction Series},
	title = {On, Above, and Beyond: Taking Tabletops to the Third Dimension},
	shorttitle = {On, Above, and Beyond},
	url = {http://link.springer.com/chapter/10.1007/978-1-84996-113-4_12},
	abstract = {Extending the tabletop to the third dimension has the potential to improve the quality of applications involving 3D data and tasks. Recognizing this, a number of researchers have proposed a myriad of display and input metaphors. However a standardized and cohesive approach has yet to evolve. Furthermore, the majority of these applications and the related research results are scattered across various research areas and communities, and lack a common framework. In this chapter, we survey previous 3D tabletops systems, and classify this work within a newly defined taxonomy. We then discuss the design guidelines which should be applied to the various areas of the taxonomy. Our contribution is the synthesis of numerous research results into a cohesive framework, and the discussion of interaction issues and design guidelines which apply. Furthermore, our work provides a clear understanding of what approaches have been taken, and exposes new routes for potential research, within the realm of interactive 3D tabletops.},
	urldate = {2014-03-05},
	booktitle = {Tabletops-Horizontal Interactive Displays},
	publisher = {Springer},
	author = {Grossman, Tovi and Wigdor, Daniel},
	year = {2010},
	pages = {277--299},
	file = {chp%3A10.1007%2F978-1-84996-113-4_12.pdf:E\:\\Papers\\Zotero Repository\\storage\\5HF83TD3\\chp%3A10.1007%2F978-1-84996-113-4_12.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\EPF92CEW\\978-1-84996-113-4_12.html:text/html}
}

@incollection{seo_enhancing_2011,
	series = {Lecture Notes in Computer Science},
	title = {Enhancing Marker-Based {AR} Technology},
	copyright = {©2011 Springer-Verlag {GmbH} Berlin Heidelberg},
	isbn = {978-3-642-22020-3, 978-3-642-22021-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-22021-0_12},
	abstract = {In this paper, we propose a method that solves both jittering and occlusion problems which is the biggest issue in marker based augmented reality technology. Because we adjust the pose estimation by using multiple keypoints that exist in the marker based on cells, we can predict the strong pose on jittering. Additionally, we can solve the occlusion problem by applying tracking technology.},
	number = {6773},
	urldate = {2014-03-05},
	booktitle = {Virtual and Mixed Reality - New Trends},
	publisher = {Springer Berlin Heidelberg},
	author = {Seo, Jonghoon and Shim, Jinwook and Choi, Ji Hye and Park, James and Han, Tack-Don},
	editor = {Shumaker, Randall},
	month = jan,
	year = {2011},
	keywords = {Artificial Intelligence (incl. Robotics), augmented reality, computer graphics, Information Systems Applications (incl.Internet), Marker-based {AR}, Multimedia Information Systems, Special Purpose and Application-Based Systems, tracking, User Interfaces and Human Computer Interaction},
	pages = {97--104},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\N4WRQVWR\\Seo et al. - 2011 - Enhancing Marker-Based AR Technology.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\NDTTVNV5\\10.html:text/html}
}

@inproceedings{steimle_flexpad:_2013,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '13},
	title = {Flexpad: Highly Flexible Bending Interactions for Projected Handheld Displays},
	isbn = {978-1-4503-1899-0},
	shorttitle = {Flexpad},
	url = {http://doi.acm.org/10.1145/2470654.2470688},
	doi = {10.1145/2470654.2470688},
	abstract = {Flexpad is an interactive system that combines a depth camera and a projector to transform sheets of plain paper or foam into flexible, highly deformable, and spatially aware handheld displays. We present a novel approach for tracking deformed surfaces from depth images in real time. It captures deformations in high detail, is very robust to occlusions created by the user's hands and fingers, and does not require any kind of markers or visible texture. As a result, the display is considerably more deformable than in previous work on flexible handheld displays, enabling novel applications that leverage the high expressiveness of detailed deformation. We illustrate these unique capabilities through three application examples: curved cross-cuts in volumetric images, deforming virtual paper characters, and slicing through time in videos. Results from two user studies show that our system is capable of detecting complex deformations and that users are able to perform them quickly and precisely.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Steimle, Jürgen and Jordt, Andreas and Maes, Pattie},
	year = {2013},
	keywords = {bending, deformation, depth camera, flexible display, handheld display, projection, tracking, volumetric data},
	pages = {237--246},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\57BTKCDA\\Steimle et al. - 2013 - Flexpad Highly Flexible Bending Interactions for .pdf:application/pdf}
}

@inproceedings{behzadan_visualization_2005,
	address = {Orlando, Florida},
	series = {{WSC} '05},
	title = {Visualization of Construction Graphics in Outdoor Augmented Reality},
	isbn = {0-7803-9519-0},
	url = {http://dl.acm.org/citation.cfm?id=1162708.1163041},
	abstract = {This paper describes research that investigates the application of Augmented Reality ({AR}) in 3D animation of simulated construction operations. The objective is an {AR}-based platform that can be used together with corresponding equipment ({HMD}, {GPS} receiver, and a portable computer) to generate a mixed view of the real world and superimposed virtual simulation objects in an outdoor environment. The characteristic that distinguishes the presented work from indoor {AR} applications is the capability to produce real time updated output as the user moves around while applying minimum constraints over the user's position and orientation. The ability to operate independently of environmental factors (e.g. lighting conditions and terrain variations) makes the described framework a powerful tool for outdoor {AR} applications. This paper presents initial results and an {AR} platform prototype ({UMAR}-{GPS}-{ROVER}) that is able to place 3D graphical objects at any desired location in outdoor augmented space.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 37th Conference on Winter Simulation},
	publisher = {Winter Simulation Conference},
	author = {Behzadan, Amir H. and Kamat, Vineet R.},
	year = {2005},
	pages = {1914--1920},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\9TNECIP4\\Behzadan 그리고 Kamat - 2005 - Visualization of Construction Graphics in Outdoor .pdf:application/pdf}
}

@inproceedings{kim_ar_2013,
	address = {New York, {NY}, {USA}},
	series = {{CHI} {EA} '13},
	title = {{AR} Pen and Hand Gestures: A New Tool for Pen Drawings},
	isbn = {978-1-4503-1952-2},
	shorttitle = {{AR} Pen and Hand Gestures},
	url = {http://doi.acm.org/10.1145/2468356.2468525},
	doi = {10.1145/2468356.2468525},
	abstract = {This paper explores the interaction possibilities when artists use their non-dominant hand, while drawing with a pen in their dominant hand. We propose a new interactive {AR}-based pen tool which can overlay virtual images onto a physical drawing in real time. This system allows artists to control the augmented images with gestures of a non-dominant hand while drawing. By interacting with the visually augmented contents using hand gestures and a pen bimanually, artists can draw pictures more creatively. We also made a standalone pen system integrated with a pico-projector and a camera, and suggest a set of useful scenarios for the conventional pen-and-paper drawing.},
	urldate = {2014-03-05},
	booktitle = {{CHI} '13 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Kim, Hark-Joon and Kim, Hayoung and Chae, Seungho and Seo, Jonghoon and Han, Tack-Don},
	year = {2013},
	keywords = {bimanual interaction, hand gesture, pen-based computing, projection-based augmented reality},
	pages = {943--948},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\FPB96PA3\\Kim et al. - 2013 - AR Pen and Hand Gestures A New Tool for Pen Drawi.pdf:application/pdf}
}

@inproceedings{schwerdtfeger_using_2008,
	address = {New York, {NY}, {USA}},
	series = {{VRST} '08},
	title = {Using Laser Projectors for Augmented Reality},
	isbn = {978-1-59593-951-7},
	url = {http://doi.acm.org/10.1145/1450579.1450608},
	doi = {10.1145/1450579.1450608},
	abstract = {The paper explores the use of laser projectors as an alternative to head-mounted displays for Augmented Reality. We describe the development of an Augmented Reality Laser Projector and report on experiences setting up {AR} systems that use laser projectors, reasoning about several design criteria.},
	urldate = {2014-03-06},
	booktitle = {Proceedings of the 2008 {ACM} Symposium on Virtual Reality Software and Technology},
	publisher = {{ACM}},
	author = {Schwerdtfeger, Björn and Pustka, Daniel and Hofhauser, Andreas and Klinker, Gudrun},
	year = {2008},
	keywords = {augmented reality, industrial augmented reality, laser projector},
	pages = {134--137},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\98WES2MH\\Schwerdtfeger et al. - 2008 - Using Laser Projectors for Augmented Reality.pdf:application/pdf}
}

@inproceedings{wilson_playanywhere:_2005,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '05},
	title = {{PlayAnywhere}: A Compact Interactive Tabletop Projection-vision System},
	isbn = {1-59593-271-2},
	shorttitle = {{PlayAnywhere}},
	url = {http://doi.acm.org/10.1145/1095034.1095047},
	doi = {10.1145/1095034.1095047},
	abstract = {We introduce {PlayAnywhere}, a front-projected computer vision-based interactive table system which uses a new commercially available projection technology to obtain a compact, self-contained form factor. {PlayAnywhere}'s configuration addresses installation, calibration, and portability issues that are typical of most vision-based table systems, and thereby is particularly motivated in consumer applications. {PlayAnywhere} also makes a number of contributions related to image processing techniques for front-projected vision-based table systems, including a shadow-based touch detection algorithm, a fast, simple visual bar code scheme tailored to projection-vision table systems, the ability to continuously track sheets of paper, and an optical flow-based algorithm for the manipulation of onscreen objects that does not rely on fragile tracking algorithms.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 18th Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Wilson, Andrew D.},
	year = {2005},
	pages = {83--92},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\8J68VJQ7\\Wilson - 2005 - PlayAnywhere A Compact Interactive Tabletop Proje.pdf:application/pdf}
}

@inproceedings{bae_ilovesketch:_2008,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '08},
	title = {{ILoveSketch}: As-natural-as-possible Sketching System for Creating 3D Curve Models},
	isbn = {978-1-59593-975-3},
	shorttitle = {{ILoveSketch}},
	url = {http://doi.acm.org/10.1145/1449715.1449740},
	doi = {10.1145/1449715.1449740},
	abstract = {We present {ILoveSketch}, a 3D curve sketching system that captures some of the affordances of pen and paper for professional designers, allowing them to iterate directly on concept 3D curve models. The system coherently integrates existing techniques of sketch-based interaction with a number of novel and enhanced features. Novel contributions of the system include automatic view rotation to improve curve sketchability, an axis widget for sketch surface selection, and implicitly inferred changes between sketching techniques. We also improve on a number of existing ideas such as a virtual sketchbook, simplified 2D and 3D view navigation, multi-stroke {NURBS} curve creation, and a cohesive gesture vocabulary. An evaluation by a professional designer shows the potential of our system for deployment within a real design process.},
	urldate = {2014-02-07},
	booktitle = {Proceedings of the 21st Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Bae, Seok-Hyung and Balakrishnan, Ravin and Singh, Karan},
	year = {2008},
	keywords = {3d curve, axis widget, implicit mode change, product design, sketchability, sketch-based modeling},
	pages = {151--160},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\2QAKUUGZ\\Bae et al. - 2008 - ILoveSketch As-natural-as-possible Sketching Syst.pdf:application/pdf}
}

@inproceedings{jones_illumiroom:_2013,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '13},
	title = {{IllumiRoom}: Peripheral Projected Illusions for Interactive Experiences},
	isbn = {978-1-4503-1899-0},
	shorttitle = {{IllumiRoom}},
	url = {http://doi.acm.org/10.1145/2470654.2466112},
	doi = {10.1145/2470654.2466112},
	abstract = {{IllumiRoom} is a proof-of-concept system that augments the area surrounding a television with projected visualizations to enhance traditional gaming experiences. We investigate how projected visualizations in the periphery can negate, include, or augment the existing physical environment and complement the content displayed on the television screen. Peripheral projected illusions can change the appearance of the room, induce apparent motion, extend the field of view, and enable entirely new physical gaming experiences. Our system is entirely self-calibrating and is designed to work in any room. We present a detailed exploration of the design space of peripheral projected illusions and we demonstrate ways to trigger and drive such illusions from gaming content. We also contribute specific feedback from two groups of target users (10 gamers and 15 game designers); providing insights for enhancing game experiences through peripheral projected illusions.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Jones, Brett R. and Benko, Hrvoje and Ofek, Eyal and Wilson, Andrew D.},
	year = {2013},
	keywords = {apparent motion, augmented reality, gaming, immersion, projection mapping, spatial augmented reality},
	pages = {869--878},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\SH6ACAU3\\Jones et al. - 2013 - IllumiRoom Peripheral Projected Illusions for Int.pdf:application/pdf}
}

@inproceedings{brandl_combining_2008,
	address = {New York, {NY}, {USA}},
	series = {{AVI} '08},
	title = {Combining and Measuring the Benefits of Bimanual Pen and Direct-touch Interaction on Horizontal Interfaces},
	isbn = {978-1-60558-141-5},
	url = {http://doi.acm.org/10.1145/1385569.1385595},
	doi = {10.1145/1385569.1385595},
	abstract = {Many research projects have demonstrated the benefits of bimanual interaction for a variety of tasks. When choosing bimanual input, system designers must select the input device that each hand will control. In this paper, we argue for the use of pen and touch two-handed input, and describe an experiment in which users were faster and committed fewer errors using pen and touch input in comparison to using either touch and touch or pen and pen input while performing a representative bimanual task. We present design principles and an application in which we applied our design rationale toward the creation of a learnable set of bimanual, pen and touch input commands.},
	urldate = {2014-02-07},
	booktitle = {Proceedings of the Working Conference on Advanced Visual Interfaces},
	publisher = {{ACM}},
	author = {Brandl, Peter and Forlines, Clifton and Wigdor, Daniel and Haller, Michael and Shen, Chia},
	year = {2008},
	keywords = {bimanual input, pen and touch, self revealing gestures},
	pages = {154--161},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\SN6X3VIW\\Brandl et al. - 2008 - Combining and Measuring the Benefits of Bimanual P.pdf:application/pdf}
}

@misc{autodesk_autocad_????,
	title = {{AutoCAD}},
	url = {http://www.autodesk.com/products/autodesk-autocad/overview},
	urldate = {2014-03-06},
	author = {Autodesk},
	note = {http://www.autodesk.com/products/autodesk-autocad/overview},
	file = {CAD Software | CAD Design | AutoCAD for Mac & Windows:E\:\\Papers\\Zotero Repository\\storage\\MVF8QE9J\\overview.html:text/html}
}

@article{kim_interactive_2012,
	title = {Interactive Modeler for Construction Equipment Operation Using Augmented Reality},
	volume = {26},
	issn = {0887-3801},
	url = {http://ascelibrary.org/doi/abs/10.1061/%28ASCE%29CP.1943-5487.0000137},
	doi = {10.1061/(ASCE)CP.1943-5487.0000137},
	abstract = {Abstract To ensure an efficient and safe construction operation, efforts have been made to develop a planning tool that focuses on equipment utilization. With the development of augmented reality ({AR}) technology came an opportunity for collaborative and interactive scenario modelling of construction equipment operation. This paper presents a system for identifying the optimum scenario for equipment operation by intuitively operating the equipment in an {AR} environment. Augmented reality was coupled with transmission control protocol/internet protocol ({TCP}/{IP}) socket programming to form an interactive interface for multiple users. In this system, users can develop a construction scenario involving equipment operation and site conditions such as project progress and share the idea with other users in distant locations. The interactive modeler can test various situations to find the particular scenario that works the best under the surrounding spatial constraints. A case study involving construction of a real cable-stayed bridge shows that the system has strong potential for significant improvement in construction planning processes.},
	number = {3},
	urldate = {2014-02-11},
	journal = {Journal of Computing in Civil Engineering},
	author = {Kim, B. and Kim, C. and Kim, H.},
	year = {2012},
	pages = {331--341},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\AHGV3P2D\\Kim et al. - 2012 - Interactive Modeler for Construction Equipment Ope.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\I8C9K8GW\\(ASCE)CP.1943-5487.html:text/html}
}

@inproceedings{cao_interacting_2006,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '06},
	title = {Interacting with Dynamically Defined Information Spaces Using a Handheld Projector and a Pen},
	isbn = {1-59593-313-1},
	url = {http://doi.acm.org/10.1145/1166253.1166289},
	doi = {10.1145/1166253.1166289},
	abstract = {The recent trend towards miniaturization of projection technology indicates that handheld devices will soon have the ability to project information onto any surface, thus enabling interfaces that are not possible with current handhelds. We explore the design space of dynamically defining and interacting with multiple virtual information spaces embedded in a physical environment using a handheld projector and a passive pen tracked in 3D. We develop techniques for defining and interacting with these spaces, and explore usage scenarios.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 19th Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Cao, Xiang and Balakrishnan, Ravin},
	year = {2006},
	keywords = {handheld projector, information spaces, pen input},
	pages = {225--234},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\4KZN2HDC\\Cao 그리고 Balakrishnan - 2006 - Interacting with Dynamically Defined Information S.pdf:application/pdf}
}

@inproceedings{benko_miragetable:_2012,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '12},
	title = {{MirageTable}: Freehand Interaction on a Projected Augmented Reality Tabletop},
	isbn = {978-1-4503-1015-4},
	shorttitle = {{MirageTable}},
	url = {http://doi.acm.org/10.1145/2207676.2207704},
	doi = {10.1145/2207676.2207704},
	abstract = {Instrumented with a single depth camera, a stereoscopic projector, and a curved screen, {MirageTable} is an interactive system designed to merge real and virtual worlds into a single spatially registered experience on top of a table. Our depth camera tracks the user's eyes and performs a real-time capture of both the shape and the appearance of any object placed in front of the camera (including user's body and hands). This real-time capture enables perspective stereoscopic 3D visualizations to a single user that account for deformations caused by physical objects on the table. In addition, the user can interact with virtual objects through physically-realistic freehand actions without any gloves, trackers, or instruments. We illustrate these unique capabilities through three application examples: virtual 3D model creation, interactive gaming with real and virtual objects, and a 3D teleconferencing experience that not only presents a 3D view of a remote person, but also a seamless 3D shared task space. We also evaluated the user's perception of projected 3D objects in our system, which confirmed that the users can correctly perceive such objects even when they are projected over different background colors and geometries (e.g., gaps, drops).},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Benko, Hrvoje and Jota, Ricardo and Wilson, Andrew},
	year = {2012},
	keywords = {3d digitization, 3d interaction, 3d teleconferencing, depth camera, projective textures, projector-camera system, shared task space, spatial augmented reality},
	pages = {199--208},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\JK7D6M9G\\Benko et al. - 2012 - MirageTable Freehand Interaction on a Projected A.pdf:application/pdf}
}

@article{behzadan_ubiquitous_2008,
	title = {Ubiquitous location tracking for context-specific information delivery on construction sites},
	volume = {17},
	issn = {0926-5805},
	url = {http://www.sciencedirect.com/science/article/pii/S0926580508000186},
	doi = {10.1016/j.autcon.2008.02.002},
	abstract = {Construction projects are information-intensive in nature and require site personnel to have continuous on-demand access to information such as project plans, drawings, schedules, and budgets. Awareness of a user's context (such as user profile, role, preferences, task, and existing project conditions) can enhance the construction project delivery process by providing a mechanism to determine information relevant to a particular context. Context awareness can also be used to improve security, logistics and health and safety practices on construction sites. Location is an important aspect of context awareness. A location aware application can utilize the knowledge of the user/object location to provide relevant information and services. This paper argues that a successful and reliable location tracking system must be able to track a user's spatial context and deliver contextual data continuously in both outdoor and indoor environments to effectively support construction projects. Research describing the use of Wireless Local Area Network ({WLAN}) for indoor tracking and Global Positioning System ({GPS}) for outdoor spatial context tracking is presented, and an integrated tracking technique using {WLAN} and {GPS} for ubiquitous location sensing is introduced. The key benefits and technical challenges of such an integrated approach are also highlighted. The presented tracking techniques have been validated in both indoor and outdoor environments to ensure their practical implementation on real construction jobsites.},
	number = {6},
	urldate = {2014-03-12},
	journal = {Automation in Construction},
	author = {Behzadan, Amir H. and Aziz, Zeeshan and Anumba, Chimay J. and Kamat, Vineet R.},
	month = aug,
	year = {2008},
	keywords = {augmented reality, Construction, Context awareness, Information delivery, Location tracking},
	pages = {737--748},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\K4RCUXQA\\Behzadan et al. - 2008 - Ubiquitous location tracking for context-specific .pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\2VR9VF72\\S0926580508000186.html:text/html}
}

@inproceedings{weiss_benddesk:_2010,
	address = {New York, {NY}, {USA}},
	series = {{ITS} '10},
	title = {{BendDesk}: Dragging Across the Curve},
	isbn = {978-1-4503-0399-6},
	shorttitle = {{BendDesk}},
	url = {http://doi.acm.org/10.1145/1936652.1936654},
	doi = {10.1145/1936652.1936654},
	abstract = {We present {BendDesk}, a hybrid interactive desk system that combines a horizontal and a vertical interactive surface via a curve. The system provides seamless touch input across its entire area. We explain scalable algorithms that provide graphical output and multi-touch input on a curved surface. In three tasks we investigate the performance of dragging gestures across the curve, as well as the virtual aiming at targets. Our main findings are: 1) Dragging across a curve is significantly slower than on flat surfaces. 2) The smaller the entrance angle when dragging across the curve, the longer the average trajectory and the higher the variance of trajectories across users. 3) The curved shape of the system impairs virtual aiming at targets.},
	urldate = {2014-03-12},
	booktitle = {{ACM} International Conference on Interactive Tabletops and Surfaces},
	publisher = {{ACM}},
	author = {Weiss, Malte and Voelker, Simon and Sutter, Christine and Borchers, Jan},
	year = {2010},
	keywords = {curved surface, desk environment, dragging, multi-touch, virtual aiming},
	pages = {1--10},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\IAFR4VQA\\Weiss et al. - 2010 - BendDesk Dragging Across the Curve.pdf:application/pdf}
}

@misc{google_sketchup_????,
	title = {{SketchUp}},
	url = {http://www.sketchup.com/},
	urldate = {2014-03-06},
	author = {Google},
	note = {http://www.sketchup.com/},
	file = {SketchUp | 3D for Everyone:E\:\\Papers\\Zotero Repository\\storage\\AJU9CZF3\\www.sketchup.com.html:text/html}
}

@inproceedings{vogel_shift:_2007,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '07},
	title = {Shift: A Technique for Operating Pen-based Interfaces Using Touch},
	isbn = {978-1-59593-593-9},
	shorttitle = {Shift},
	url = {http://doi.acm.org/10.1145/1240624.1240727},
	doi = {10.1145/1240624.1240727},
	abstract = {Retrieving the stylus of a pen-based device takes time and requires a second hand. Especially for short intermittent interactions many users therefore choose to use their bare fingers. Although convenient, this increases targeting times and error rates. We argue that the main reasons are the occlusion of the target by the user's finger and ambiguity about which part of the finger defines the selection point. We propose a pointing technique we call Shift that is designed to address these issues. When the user touches the screen, Shift creates a callout showing a copy of the occluded screen area and places it in a non-occluded location. The callout also shows a pointer representing the selection point of the finger. Using this visual feedback, users guide the pointer into the target by moving their finger on the screen surface and commit the target acquisition by lifting the finger. Unlike existing techniques, Shift is only invoked when necessary--over large targets no callout is created and users enjoy the full performance of an unaltered touch screen. We report the results of a user study showing that with Shift participants can select small targets with much lower error rates than an unaided touch screen and that Shift is faster than Offset Cursor for larger targets.},
	urldate = {2014-09-22},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Vogel, Daniel and Baudisch, Patrick},
	year = {2007},
	keywords = {interaction techniques, mobile devices, occlusion, precise target acquisition, touch-screens},
	pages = {657--666},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\IRK2B4H5\\Vogel 그리고 Baudisch - 2007 - Shift A Technique for Operating Pen-based Interfa.pdf:application/pdf}
}

@inproceedings{harrison_omnitouch:_2011,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '11},
	title = {{OmniTouch}: Wearable Multitouch Interaction Everywhere},
	isbn = {978-1-4503-0716-1},
	shorttitle = {{OmniTouch}},
	url = {http://doi.acm.org/10.1145/2047196.2047255},
	doi = {10.1145/2047196.2047255},
	abstract = {{OmniTouch} is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - {OmniTouch} provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are "clicked" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 24th Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Harrison, Chris and Benko, Hrvoje and Wilson, Andrew D.},
	year = {2011},
	keywords = {appropriated surfaces, finger tracking, object classification, on-body computing, on-demand interfaces},
	pages = {441--450},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\MEESDCXK\\Harrison et al. - 2011 - OmniTouch Wearable Multitouch Interaction Everywh.pdf:application/pdf}
}

@inproceedings{huber_lightbeam:_2012,
	address = {New York, {NY}, {USA}},
	series = {{CHI} {EA} '12},
	title = {{LightBeam}: Nomadic Pico Projector Interaction with Real World Objects},
	isbn = {978-1-4503-1016-1},
	shorttitle = {{LightBeam}},
	url = {http://doi.acm.org/10.1145/2212776.2223828},
	doi = {10.1145/2212776.2223828},
	abstract = {Pico projectors have lately been investigated as mobile display and interaction devices. We propose to use them as 'light beams': Everyday objects sojourning in a beam are turned into dedicated projection surfaces and tangible interaction devices. While this has been explored for large projectors, the affordances of pico projectors are fundamentally different: they have a very small and strictly limited projection ray and can be carried around in a nomadic way during the day. Thus it is unclear how this could be actually leveraged for tangible interaction with physical, real world objects. We have investigated this in an exploratory field study and contribute the results. Based upon these, we present exemplary interaction techniques and early user feedback.},
	urldate = {2014-03-12},
	booktitle = {{CHI} '12 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Huber, Jochen and Steimle, Jürgen and Liao, Chunyuan and Liu, Qiong and Mühlhäuser, Max},
	year = {2012},
	keywords = {augmented reality, depth sensing, embodied interaction, exploratory study, handheld projector, kinect, mixed reality, mobile device, object tracking, pico projector, qualitative research, real world object interaction},
	pages = {2513--2518},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\REH5E6EG\\Huber et al. - 2012 - LightBeam Nomadic Pico Projector Interaction with.pdf:application/pdf}
}

@inproceedings{wright_open_1997-1,
	address = {Thessaloniki, Hellas},
	title = {Open Sound Control: A New Protocol for Communicating with Sound Synthesizers},
	url = {http://cnmat.berkeley.edu/publications/open_sound_control_new_protocol_communicating_sound_synthesizers},
	abstract = {Open {SoundControl} is a new protocol for communication among computers, sound synthesizers, and other multimedia devices that is optimized for modern networking technology. Entities within a system are addressed individually by an open-ended {URL}-style symbolic naming scheme that includes a powerful pattern matching language to specify multiple recipients of a single message. We provide high resolution time tags and a mechanism for specifying groups of messages whose effects are to occur simultaneously. There is also a mechanism for dynamically querying an Open {SoundControl} system to find out its capabilities and documentation of its features.},
	booktitle = {International Computer Music Conference},
	publisher = {International Computer Music Association},
	author = {Wright, Matthew and Freed, Adrian},
	year = {1997},
	keywords = {{OSC}},
	pages = {101--104}
}

@inproceedings{willis_hideout:_2013,
	address = {New York, {NY}, {USA}},
	series = {{TEI} '13},
	title = {{HideOut}: Mobile Projector Interaction with Tangible Objects and Surfaces},
	isbn = {978-1-4503-1898-3},
	shorttitle = {{HideOut}},
	url = {http://doi.acm.org/10.1145/2460625.2460682},
	doi = {10.1145/2460625.2460682},
	abstract = {{HideOut} is a mobile projector-based system that enables new applications and interaction techniques with tangible objects and surfaces. {HideOut} uses a device mounted camera to detect hidden markers applied with infrared-absorbing ink. The obtrusive appearance of fiducial markers is avoided and the hidden marker surface doubles as a functional projection surface. We present example applications that demonstrate a wide range of interaction scenarios, including media navigation tools, interactive storytelling applications, and mobile games. We explore the design space enabled by the {HideOut} system and describe the hidden marker prototyping process. {HideOut} brings tangible objects to life for interaction with the physical world around us.},
	urldate = {2014-03-12},
	booktitle = {Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction},
	publisher = {{ACM}},
	author = {Willis, Karl D. D. and Shiratori, Takaaki and Mahler, Moshe},
	year = {2013},
	keywords = {hidden, infrared, ink, interaction, marker, mobile, projector, tangible},
	pages = {331--338},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\4DUKA66B\\Willis et al. - 2013 - HideOut Mobile Projector Interaction with Tangibl.pdf:application/pdf}
}